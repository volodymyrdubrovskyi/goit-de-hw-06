2024-12-01 01:40:17 INFO  log:170 - Logging initialized @2902ms to org.sparkproject.jetty.util.log.Slf4jLog
2024-12-01 01:40:17 INFO  Server:375 - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 21.0.5+11-LTS
2024-12-01 01:40:17 INFO  Server:415 - Started @2997ms
2024-12-01 01:40:17 INFO  AbstractConnector:333 - Started ServerConnector@522fe2a6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-12-01 01:40:17 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@7e22924{/,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:1159 - Stopped o.s.j.s.ServletContextHandler@7e22924{/,null,STOPPED,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@5f54f2e{/jobs,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@48f72b1c{/jobs/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@462b32fd{/jobs/job,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@749e6c7e{/jobs/job/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@76531ad0{/stages,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@7e43d732{/stages/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@1857c38c{/stages/stage,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@2ddd8f70{/stages/stage/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@10369688{/stages/pool,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4e7326fb{/stages/pool/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@7a06fef{/storage,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@7bff995b{/storage/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@6dce1c18{/storage/rdd,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@276b05dd{/storage/rdd/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@7d44fd71{/environment,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@6962aa7b{/environment/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@6b25da89{/executors,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@41e2df51{/executors/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@3502f1e9{/executors/threadDump,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@237c3557{/executors/threadDump/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4546f44d{/executors/heapHistogram,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@68a86bf3{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@3319d4aa{/static,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@511ecdfd{/,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@71520b6d{/api,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@5272ba3{/jobs/job/kill,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@2122eb74{/stages/stage/kill,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@d838cc2{/metrics/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4c6da4aa{/SQL,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@a8355a5{/SQL/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@394aa87e{/SQL/execution,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@8c951af{/SQL/execution/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:21 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@76cffa3f{/static/sql,null,AVAILABLE,@Spark}
2024-12-01 01:40:23 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4555802b{/StreamingQuery,null,AVAILABLE,@Spark}
2024-12-01 01:40:23 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@1fbf8a7f{/StreamingQuery/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:23 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@1dafa711{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2024-12-01 01:40:23 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@37d3e970{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2024-12-01 01:40:23 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4aba4796{/static/sql,null,AVAILABLE,@Spark}
2024-12-01 01:40:24 INFO  AdminClientConfig:372 - AdminClientConfig values: 
	bootstrap.servers = [77.81.230.104:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = PLAIN
	security.protocol = SASL_PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2024-12-01 01:40:24 INFO  AbstractLogin:61 - Successfully logged in.
2024-12-01 01:40:24 WARN  AdminClientConfig:380 - The configuration 'key.deserializer' was supplied but isn't a known config.
2024-12-01 01:40:24 WARN  AdminClientConfig:380 - The configuration 'value.deserializer' was supplied but isn't a known config.
2024-12-01 01:40:24 WARN  AdminClientConfig:380 - The configuration 'enable.auto.commit' was supplied but isn't a known config.
2024-12-01 01:40:24 WARN  AdminClientConfig:380 - The configuration 'max.poll.records' was supplied but isn't a known config.
2024-12-01 01:40:24 WARN  AdminClientConfig:380 - The configuration 'auto.offset.reset' was supplied but isn't a known config.
2024-12-01 01:40:24 INFO  AppInfoParser:119 - Kafka version: 2.8.0
2024-12-01 01:40:24 INFO  AppInfoParser:120 - Kafka commitId: ebb1d6e21cc92130
2024-12-01 01:40:24 INFO  AppInfoParser:121 - Kafka startTimeMs: 1733010024157
2024-12-01 01:40:24 INFO  AdminClientConfig:372 - AdminClientConfig values: 
	bootstrap.servers = [77.81.230.104:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = PLAIN
	security.protocol = SASL_PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2024-12-01 01:40:24 WARN  AdminClientConfig:380 - The configuration 'key.deserializer' was supplied but isn't a known config.
2024-12-01 01:40:24 WARN  AdminClientConfig:380 - The configuration 'value.deserializer' was supplied but isn't a known config.
2024-12-01 01:40:24 WARN  AdminClientConfig:380 - The configuration 'enable.auto.commit' was supplied but isn't a known config.
2024-12-01 01:40:24 WARN  AdminClientConfig:380 - The configuration 'max.poll.records' was supplied but isn't a known config.
2024-12-01 01:40:24 WARN  AdminClientConfig:380 - The configuration 'auto.offset.reset' was supplied but isn't a known config.
2024-12-01 01:40:24 INFO  AppInfoParser:119 - Kafka version: 2.8.0
2024-12-01 01:40:24 INFO  AppInfoParser:120 - Kafka commitId: ebb1d6e21cc92130
2024-12-01 01:40:24 INFO  AppInfoParser:121 - Kafka startTimeMs: 1733010024451
2024-12-01 01:40:28 INFO  ConsumerConfig:372 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [77.81.230.104:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = PLAIN
	security.protocol = SASL_PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2024-12-01 01:40:28 INFO  ConsumerConfig:372 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [77.81.230.104:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = PLAIN
	security.protocol = SASL_PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2024-12-01 01:40:28 INFO  AppInfoParser:119 - Kafka version: 2.8.0
2024-12-01 01:40:28 INFO  AppInfoParser:120 - Kafka commitId: ebb1d6e21cc92130
2024-12-01 01:40:28 INFO  AppInfoParser:121 - Kafka startTimeMs: 1733010028568
2024-12-01 01:40:28 INFO  AppInfoParser:119 - Kafka version: 2.8.0
2024-12-01 01:40:28 INFO  AppInfoParser:120 - Kafka commitId: ebb1d6e21cc92130
2024-12-01 01:40:28 INFO  KafkaConsumer:1120 - [Consumer clientId=consumer-spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor-1, groupId=spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor] Subscribed to partition(s): vvd_building_sensors-0
2024-12-01 01:40:28 INFO  AppInfoParser:121 - Kafka startTimeMs: 1733010028568
2024-12-01 01:40:28 INFO  KafkaConsumer:1120 - [Consumer clientId=consumer-spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor-2, groupId=spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor] Subscribed to partition(s): vvd_building_sensors-0
2024-12-01 01:40:28 INFO  KafkaConsumer:1582 - [Consumer clientId=consumer-spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor-1, groupId=spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor] Seeking to offset 0 for partition vvd_building_sensors-0
2024-12-01 01:40:28 INFO  KafkaConsumer:1582 - [Consumer clientId=consumer-spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor-2, groupId=spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor] Seeking to offset 0 for partition vvd_building_sensors-0
2024-12-01 01:40:28 INFO  Metadata:279 - [Consumer clientId=consumer-spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor-2, groupId=spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor] Cluster ID: chIAo_gdRnOVaOI0JkkL_w
2024-12-01 01:40:28 INFO  Metadata:279 - [Consumer clientId=consumer-spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor-1, groupId=spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor] Cluster ID: chIAo_gdRnOVaOI0JkkL_w
2024-12-01 01:40:29 INFO  SubscriptionState:619 - [Consumer clientId=consumer-spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor-1, groupId=spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor] Seeking to EARLIEST offset of partition vvd_building_sensors-0
2024-12-01 01:40:29 INFO  SubscriptionState:619 - [Consumer clientId=consumer-spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor-2, groupId=spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor] Seeking to EARLIEST offset of partition vvd_building_sensors-0
2024-12-01 01:40:29 INFO  SubscriptionState:398 - [Consumer clientId=consumer-spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor-1, groupId=spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor] Resetting offset for partition vvd_building_sensors-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[77.81.230.104:9092 (id: 1001 rack: null)], epoch=0}}.
2024-12-01 01:40:29 INFO  SubscriptionState:619 - [Consumer clientId=consumer-spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor-1, groupId=spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor] Seeking to LATEST offset of partition vvd_building_sensors-0
2024-12-01 01:40:29 INFO  SubscriptionState:398 - [Consumer clientId=consumer-spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor-2, groupId=spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor] Resetting offset for partition vvd_building_sensors-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[77.81.230.104:9092 (id: 1001 rack: null)], epoch=0}}.
2024-12-01 01:40:29 INFO  SubscriptionState:619 - [Consumer clientId=consumer-spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor-2, groupId=spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor] Seeking to LATEST offset of partition vvd_building_sensors-0
2024-12-01 01:40:29 INFO  SubscriptionState:398 - [Consumer clientId=consumer-spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor-2, groupId=spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor] Resetting offset for partition vvd_building_sensors-0 to position FetchPosition{offset=82, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[77.81.230.104:9092 (id: 1001 rack: null)], epoch=0}}.
2024-12-01 01:40:29 INFO  SubscriptionState:398 - [Consumer clientId=consumer-spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor-1, groupId=spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor] Resetting offset for partition vvd_building_sensors-0 to position FetchPosition{offset=82, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[77.81.230.104:9092 (id: 1001 rack: null)], epoch=0}}.
2024-12-01 01:40:38 INFO  Metrics:659 - Metrics scheduler closed
2024-12-01 01:40:38 INFO  Metrics:663 - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-12-01 01:40:38 INFO  Metrics:669 - Metrics reporters closed
2024-12-01 01:40:38 INFO  AppInfoParser:83 - App info kafka.consumer for consumer-spark-kafka-source-66e5b473-94bb-402d-aabe-b0d8261ee37f--1227833776-executor-1 unregistered
2024-12-01 01:40:38 INFO  Metrics:659 - Metrics scheduler closed
2024-12-01 01:40:38 INFO  Metrics:663 - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-12-01 01:40:38 INFO  Metrics:669 - Metrics reporters closed
2024-12-01 01:40:38 INFO  AppInfoParser:83 - App info kafka.consumer for consumer-spark-kafka-source-1cfe449b-1454-486a-9fe6-4cc07aafe047-1015960358-executor-2 unregistered
2024-12-01 01:40:38 INFO  AbstractConnector:383 - Stopped Spark@522fe2a6{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-12-01 01:40:38 ERROR MicroBatchExecution:97 - Query [id = d59afc18-32c4-4791-b166-a009965a304c, runId = 0c19291f-f251-4f1d-be2c-bdf74c32f944] terminated with error
py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "c:\Users\vdubr\OneDrive\Документы\GoIT Обучение\Data Engineering\goit-de-hw-06\p2_5.py", line 55, in print_to_console_1
    sorted_df.show(truncate=False, n=100)
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 947, in show
    print(self._show_string(n, truncate, vertical))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 978, in _show_string
    return self._jdf.showString(n, int_truncate, vertical)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o163.showString.
: org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)
	at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2258)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2211)
	at org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)
	at org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1139)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1121)
	at org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1568)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1555)
	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$executeCollect$1(limit.scala:291)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:285)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:3316)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:3539)
	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy26.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.MultiBatchExecutor.execute(TriggerExecutor.scala:49)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)


	at py4j.Protocol.getReturnValue(Protocol.java:476) ~[py4j-0.10.9.7.jar:?]
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108) ~[py4j-0.10.9.7.jar:?]
	at jdk.proxy3.$Proxy26.call(Unknown Source) ~[?:?]
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MultiBatchExecutor.execute(TriggerExecutor.scala:49) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) [scala-library-2.12.18.jar:?]
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94) [spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211) [spark-sql_2.12-3.5.3.jar:3.5.3]
2024-12-01 01:40:38 ERROR MicroBatchExecution:97 - Query [id = 6cc67743-cfb2-4b11-8f4e-5dc4dc602b6a, runId = 1c6b900e-5d25-4e90-a672-9781bce76ec6] terminated with error
py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\utils.py", line 120, in call
    raise e
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "c:\Users\vdubr\OneDrive\Документы\GoIT Обучение\Data Engineering\goit-de-hw-06\p2_5.py", line 86, in print_to_console_2
    rows = sorted_df.collect()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\sql\dataframe.py", line 1263, in collect
    sock_info = self._jdf.collectToPython()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\Users\vdubr\AppData\Local\Programs\Python\Python311\Lib\site-packages\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o164.collectToPython.
: org.apache.spark.SparkException: Job 1 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)
	at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2258)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2211)
	at org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1583)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)
	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)
	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:296)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:179)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:173)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)
	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)
	at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:364)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:445)
	at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4149)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy26.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.MultiBatchExecutor.execute(TriggerExecutor.scala:49)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)


	at py4j.Protocol.getReturnValue(Protocol.java:476) ~[py4j-0.10.9.7.jar:?]
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108) ~[py4j-0.10.9.7.jar:?]
	at jdk.proxy3.$Proxy26.call(Unknown Source) ~[?:?]
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MultiBatchExecutor.execute(TriggerExecutor.scala:49) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211) ~[spark-sql_2.12-3.5.3.jar:3.5.3]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) [scala-library-2.12.18.jar:?]
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94) [spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211) [spark-sql_2.12-3.5.3.jar:3.5.3]
2024-12-01 01:40:38 INFO  AppInfoParser:83 - App info kafka.admin.client for adminclient-2 unregistered
2024-12-01 01:40:38 INFO  AppInfoParser:83 - App info kafka.admin.client for adminclient-1 unregistered
2024-12-01 01:40:38 INFO  Metrics:659 - Metrics scheduler closed
2024-12-01 01:40:38 INFO  Metrics:663 - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-12-01 01:40:38 INFO  Metrics:669 - Metrics reporters closed
2024-12-01 01:40:38 INFO  Metrics:659 - Metrics scheduler closed
2024-12-01 01:40:38 INFO  Metrics:663 - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-12-01 01:40:38 INFO  Metrics:669 - Metrics reporters closed
2024-12-01 01:40:38 ERROR ShutdownHookManager:97 - Exception while deleting Spark temp dir: C:\Users\vdubr\AppData\Local\Temp\spark-e78354d7-637b-4e33-b823-fe2ca37e6f74
java.io.IOException: Failed to delete: C:\Users\vdubr\AppData\Local\Temp\spark-e78354d7-637b-4e33-b823-fe2ca37e6f74\userFiles-d98f62b7-ede3-4e83-bbb6-f95fd8954191\org.xerial.snappy_snappy-java-1.1.8.4.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:147) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:130) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:130) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36) ~[scala-library-2.12.18.jar:?]
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]
	at scala.util.Try$.apply(Try.scala:213) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2024-12-01 01:40:38 ERROR Utils:97 - Uncaught exception in thread Executor task launch worker for task 22.0 in stage 3.0 (TID 4)
java.lang.NullPointerException: Cannot invoke "org.apache.spark.SparkEnv.blockManager()" because the return value of "org.apache.spark.SparkEnv$.get()" is null
	at org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:146) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.scheduler.Task.run(Task.scala:144) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2024-12-01 01:40:38 ERROR ShutdownHookManager:97 - Exception while deleting Spark temp dir: C:\Users\vdubr\AppData\Local\Temp\spark-e78354d7-637b-4e33-b823-fe2ca37e6f74\userFiles-d98f62b7-ede3-4e83-bbb6-f95fd8954191
java.io.IOException: Failed to delete: C:\Users\vdubr\AppData\Local\Temp\spark-e78354d7-637b-4e33-b823-fe2ca37e6f74\userFiles-d98f62b7-ede3-4e83-bbb6-f95fd8954191\org.xerial.snappy_snappy-java-1.1.8.4.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:147) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:130) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120) ~[spark-common-utils_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36) ~[scala-library-2.12.18.jar:?]
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33) ~[scala-library-2.12.18.jar:?]
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.18.jar:?]
	at scala.util.Try$.apply(Try.scala:213) ~[scala-library-2.12.18.jar:?]
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178) ~[spark-core_2.12-3.5.3.jar:3.5.3]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]
	at java.lang.Thread.run(Thread.java:1583) [?:?]
2024-12-01 01:40:38 ERROR Executor:76 - Exception in task 22.0 in stage 3.0 (TID 4): Cannot invoke "org.apache.spark.SparkEnv.conf()" because "env" is null
2024-12-01 01:43:39 INFO  log:170 - Logging initialized @3028ms to org.sparkproject.jetty.util.log.Slf4jLog
2024-12-01 01:43:39 INFO  Server:375 - jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 21.0.5+11-LTS
2024-12-01 01:43:39 INFO  Server:415 - Started @3146ms
2024-12-01 01:43:39 INFO  AbstractConnector:333 - Started ServerConnector@42e3fdfc{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2024-12-01 01:43:39 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@2ac16816{/,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:1159 - Stopped o.s.j.s.ServletContextHandler@2ac16816{/,null,STOPPED,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@7de09092{/jobs,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@1e732d18{/jobs/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@1cc1a51b{/jobs/job,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@e69cc8e{/jobs/job/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@6eb96720{/stages,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@7e0b7ba1{/stages/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@cb62e45{/stages/stage,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@34702372{/stages/stage/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@59891502{/stages/pool,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@562738f7{/stages/pool/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@25db8dc5{/storage,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@66e3c759{/storage/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@76571c8e{/storage/rdd,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@62c625e3{/storage/rdd/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@7b9ff61d{/environment,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@de0feba{/environment/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@2df7e44a{/executors,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@2830c0d3{/executors/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@7567277d{/executors/threadDump,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@6f5faba{/executors/threadDump/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@2ff3e4af{/executors/heapHistogram,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@42daf376{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@afcfd5f{/static,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@5242feb7{/,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@6824cbb9{/api,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4148994{/jobs/job/kill,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@c470720{/stages/stage/kill,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@d75fed1{/metrics/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@4e080ed4{/SQL,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@306d04e0{/SQL/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@21d05f13{/SQL/execution,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@12d06720{/SQL/execution/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:43 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@266ae6f4{/static/sql,null,AVAILABLE,@Spark}
2024-12-01 01:43:45 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@6e57b47e{/StreamingQuery,null,AVAILABLE,@Spark}
2024-12-01 01:43:45 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@375d2a46{/StreamingQuery/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:45 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@2e93227{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
2024-12-01 01:43:45 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@6143c738{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
2024-12-01 01:43:45 INFO  ContextHandler:921 - Started o.s.j.s.ServletContextHandler@468584cf{/static/sql,null,AVAILABLE,@Spark}
2024-12-01 01:43:45 INFO  AdminClientConfig:372 - AdminClientConfig values: 
	bootstrap.servers = [77.81.230.104:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = PLAIN
	security.protocol = SASL_PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2024-12-01 01:43:45 INFO  AbstractLogin:61 - Successfully logged in.
2024-12-01 01:43:45 WARN  AdminClientConfig:380 - The configuration 'key.deserializer' was supplied but isn't a known config.
2024-12-01 01:43:45 WARN  AdminClientConfig:380 - The configuration 'value.deserializer' was supplied but isn't a known config.
2024-12-01 01:43:45 WARN  AdminClientConfig:380 - The configuration 'enable.auto.commit' was supplied but isn't a known config.
2024-12-01 01:43:45 WARN  AdminClientConfig:380 - The configuration 'max.poll.records' was supplied but isn't a known config.
2024-12-01 01:43:45 WARN  AdminClientConfig:380 - The configuration 'auto.offset.reset' was supplied but isn't a known config.
2024-12-01 01:43:45 INFO  AppInfoParser:119 - Kafka version: 2.8.0
2024-12-01 01:43:45 INFO  AppInfoParser:120 - Kafka commitId: ebb1d6e21cc92130
2024-12-01 01:43:45 INFO  AppInfoParser:121 - Kafka startTimeMs: 1733010225972
2024-12-01 01:43:46 INFO  AdminClientConfig:372 - AdminClientConfig values: 
	bootstrap.servers = [77.81.230.104:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = PLAIN
	security.protocol = SASL_PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2024-12-01 01:43:46 WARN  AdminClientConfig:380 - The configuration 'key.deserializer' was supplied but isn't a known config.
2024-12-01 01:43:46 WARN  AdminClientConfig:380 - The configuration 'value.deserializer' was supplied but isn't a known config.
2024-12-01 01:43:46 WARN  AdminClientConfig:380 - The configuration 'enable.auto.commit' was supplied but isn't a known config.
2024-12-01 01:43:46 WARN  AdminClientConfig:380 - The configuration 'max.poll.records' was supplied but isn't a known config.
2024-12-01 01:43:46 WARN  AdminClientConfig:380 - The configuration 'auto.offset.reset' was supplied but isn't a known config.
2024-12-01 01:43:46 INFO  AppInfoParser:119 - Kafka version: 2.8.0
2024-12-01 01:43:46 INFO  AppInfoParser:120 - Kafka commitId: ebb1d6e21cc92130
2024-12-01 01:43:46 INFO  AppInfoParser:121 - Kafka startTimeMs: 1733010226228
2024-12-01 01:43:50 INFO  ConsumerConfig:372 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [77.81.230.104:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = PLAIN
	security.protocol = SASL_PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2024-12-01 01:43:50 INFO  ConsumerConfig:372 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [77.81.230.104:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = PLAIN
	security.protocol = SASL_PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2024-12-01 01:43:50 INFO  AppInfoParser:119 - Kafka version: 2.8.0
2024-12-01 01:43:50 INFO  AppInfoParser:120 - Kafka commitId: ebb1d6e21cc92130
2024-12-01 01:43:50 INFO  AppInfoParser:121 - Kafka startTimeMs: 1733010230445
2024-12-01 01:43:50 INFO  KafkaConsumer:1120 - [Consumer clientId=consumer-spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor-1, groupId=spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor] Subscribed to partition(s): vvd_building_sensors-0
2024-12-01 01:43:50 INFO  AppInfoParser:119 - Kafka version: 2.8.0
2024-12-01 01:43:50 INFO  AppInfoParser:120 - Kafka commitId: ebb1d6e21cc92130
2024-12-01 01:43:50 INFO  AppInfoParser:121 - Kafka startTimeMs: 1733010230445
2024-12-01 01:43:50 INFO  KafkaConsumer:1120 - [Consumer clientId=consumer-spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor-2, groupId=spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor] Subscribed to partition(s): vvd_building_sensors-0
2024-12-01 01:43:50 INFO  KafkaConsumer:1582 - [Consumer clientId=consumer-spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor-2, groupId=spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor] Seeking to offset 0 for partition vvd_building_sensors-0
2024-12-01 01:43:50 INFO  KafkaConsumer:1582 - [Consumer clientId=consumer-spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor-1, groupId=spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor] Seeking to offset 0 for partition vvd_building_sensors-0
2024-12-01 01:43:50 INFO  Metadata:279 - [Consumer clientId=consumer-spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor-2, groupId=spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor] Cluster ID: chIAo_gdRnOVaOI0JkkL_w
2024-12-01 01:43:50 INFO  Metadata:279 - [Consumer clientId=consumer-spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor-1, groupId=spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor] Cluster ID: chIAo_gdRnOVaOI0JkkL_w
2024-12-01 01:43:51 INFO  SubscriptionState:619 - [Consumer clientId=consumer-spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor-2, groupId=spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor] Seeking to EARLIEST offset of partition vvd_building_sensors-0
2024-12-01 01:43:51 INFO  SubscriptionState:619 - [Consumer clientId=consumer-spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor-1, groupId=spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor] Seeking to EARLIEST offset of partition vvd_building_sensors-0
2024-12-01 01:43:51 INFO  SubscriptionState:398 - [Consumer clientId=consumer-spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor-2, groupId=spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor] Resetting offset for partition vvd_building_sensors-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[77.81.230.104:9092 (id: 1001 rack: null)], epoch=0}}.
2024-12-01 01:43:51 INFO  SubscriptionState:398 - [Consumer clientId=consumer-spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor-1, groupId=spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor] Resetting offset for partition vvd_building_sensors-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[77.81.230.104:9092 (id: 1001 rack: null)], epoch=0}}.
2024-12-01 01:43:51 INFO  SubscriptionState:619 - [Consumer clientId=consumer-spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor-2, groupId=spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor] Seeking to LATEST offset of partition vvd_building_sensors-0
2024-12-01 01:43:51 INFO  SubscriptionState:619 - [Consumer clientId=consumer-spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor-1, groupId=spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor] Seeking to LATEST offset of partition vvd_building_sensors-0
2024-12-01 01:43:51 INFO  SubscriptionState:398 - [Consumer clientId=consumer-spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor-2, groupId=spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor] Resetting offset for partition vvd_building_sensors-0 to position FetchPosition{offset=82, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[77.81.230.104:9092 (id: 1001 rack: null)], epoch=0}}.
2024-12-01 01:43:51 INFO  SubscriptionState:398 - [Consumer clientId=consumer-spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor-1, groupId=spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor] Resetting offset for partition vvd_building_sensors-0 to position FetchPosition{offset=82, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[77.81.230.104:9092 (id: 1001 rack: null)], epoch=0}}.
2024-12-01 01:43:55 INFO  AppInfoParser:83 - App info kafka.admin.client for adminclient-1 unregistered
2024-12-01 01:43:55 INFO  Metrics:659 - Metrics scheduler closed
2024-12-01 01:43:55 INFO  Metrics:663 - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-12-01 01:43:55 INFO  Metrics:669 - Metrics reporters closed
2024-12-01 01:44:01 INFO  AppInfoParser:83 - App info kafka.admin.client for adminclient-2 unregistered
2024-12-01 01:44:01 INFO  Metrics:659 - Metrics scheduler closed
2024-12-01 01:44:01 INFO  Metrics:663 - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-12-01 01:44:01 INFO  Metrics:669 - Metrics reporters closed
2024-12-01 01:44:01 INFO  Metrics:659 - Metrics scheduler closed
2024-12-01 01:44:01 INFO  Metrics:663 - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-12-01 01:44:01 INFO  Metrics:669 - Metrics reporters closed
2024-12-01 01:44:01 INFO  AppInfoParser:83 - App info kafka.consumer for consumer-spark-kafka-source-98464fe9-84a3-4461-8b06-f25a6bf7268c-817827067-executor-2 unregistered
2024-12-01 01:44:01 INFO  Metrics:659 - Metrics scheduler closed
2024-12-01 01:44:01 INFO  Metrics:663 - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2024-12-01 01:44:01 INFO  Metrics:669 - Metrics reporters closed
2024-12-01 01:44:01 INFO  AppInfoParser:83 - App info kafka.consumer for consumer-spark-kafka-source-f0f0eeaf-3c29-493c-bf84-cbe1b9d1b8c0-1836220058-executor-1 unregistered
